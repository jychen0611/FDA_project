# -*- coding: utf-8 -*-
"""Aicpu_Spring(Elian) (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a5Ju7J3l0UN8iKAxalWuia8C2mrOTXEM

# Preparation
---
1. Make a copy of this notebook

<img src="https://i.imgur.com/gnEL1fM.jpg" alt="drawing" width="600"/>

2. Login to your google account and open **[this link](https://drive.google.com/drive/folders/1zuuk7D9qCNHZzDwwZT4fbcFjlWn2fl1O?usp=sharing)**

3. You should see this

<img src="https://i.imgur.com/hBQaYo0.jpg" alt="drawing" width="600"/>

4. Make a shortcut to your drive

<img src="https://i.imgur.com/8TETvok.png" alt="drawing" width="600"/>

5. Make sure link to "My Drive"

<img src="https://imgur.com/2hXPSZF.jpg" alt="drawing" width="600"/>
"""

from google.colab import drive
import sys
import os
drive.mount('/content/gdrive/')
if not os.path.exists('/content/data'):
    !cp -r /content/gdrive/MyDrive/Aicup_spring1/data /content  
if not os.path.exists('/content/dataset'):
    !cp -r /content/gdrive/MyDrive/Aicup_spring1/dataset /content

"""# The Encoder from Transformer
---
paper link: \\
[Hierarchical Attention Networks for Document Classification](https://www.aclweb.org/anthology/N16-1174.pdf) \\
[Attention Is All You Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)


"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math


class PositionalEncoding(nn.Module):
    def __init__(self, d_emb: int, dropout: float = 0.1, max_len: int = 200):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        # Compute the positional encodings once in log space.
        self.pe = torch.zeros(max_len, d_emb)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_emb, 2) *
                             -(math.log(10000.0) / d_emb))
        self.pe[:, 0::2] = torch.sin(position * div_term)
        self.pe[:, 1::2] = torch.cos(position * div_term)
        self.pe = self.pe.unsqueeze(0)

    def forward(self, src):
        pe = self.pe.detach().to(src.device)
        output = src + pe[:, :src.size(1)]
        return self.dropout(output)


class MultiHeadAttention(nn.Module):
    def __init__(self, d_hid, n_head):
        super(MultiHeadAttention, self).__init__()
        self.n_head = n_head
        self.d_k = d_hid // n_head
        self.query = nn.Parameter(torch.randn(1, d_hid))
        self.key = nn.Linear(d_hid, d_hid)
        self.value = nn.Linear(d_hid, d_hid)
        self.linear = nn.Linear(d_hid, d_hid)

    def forward(self, x, batch_tk_mask):
        B = x.size(0)

        # Input  shape: `(1, Hid)`.
        # Output shape: `(Head, 1, K)`.
        q = self.query.view(-1, self.n_head, self.d_k).transpose(0, 1)

        # Transform temporal features to query, key and value features.
        # Input  shape: `(B, S, Hid)`.
        # Output shape: `(B, Head, S, K)`.
        k = self.key(x).view(B, -1, self.n_head, self.d_k).transpose(1, 2)
        v = self.value(x).view(B, -1, self.n_head, self.d_k).transpose(1, 2)

        # Calculate self attention scores with query and key features.
        # Self attention scores are scaled down by hidden dimension square
        # root to avoid overflow.
        # `q` Input  shape: `(Head, 1, K)`.
        # `k` Input  shape: `(B, Head, S, K)`.
        # Output shape: `(B, Head, 1, S)`.
        # print(q.shape)
        # print(k.shape)
        attn = q @ k.transpose(-1, -2) / math.sqrt(x.size(-1))

        # Mask parts of attention scores by replacing with large negative
        # values.
        # Input  shape: `(B, Head, 1, S)`.
        # Output shape: `(B, Head, 1, S)`.
        batch_tk_mask = batch_tk_mask.repeat(self.n_head, 1, 1, 1)

        batch_tk_mask = batch_tk_mask.transpose(0, 1)
        batch_tk_mask = batch_tk_mask.transpose(-1, -2)
        # print(attn.shape)
        # print(batch_tk_mask.shape)
        attn.masked_fill_(batch_tk_mask, -1e9)

        # Softmax normalize on attention scores.
        # Large negative values will be closed to zero after normalization.
        # Input  shape: `(B, Head, 1, S)`.
        # Output shape: `(B, Head, 1, S)`.
        attn = F.softmax(attn, dim=-1)

        # Use attention scores to calculate weighted sum on value features.
        # Then perform one more linear tranformation on weighted sum.
        # Finally dropout transformed features.
        # `attn` Input  shape: `(B, Head, 1, S)`.
        # `v` Input  shape: `(B, Head, S, k)`.
        # Output shape: `(B, Head, 1, K)`.
        output = attn @ v

        # Input  shape: `(B, Head, 1, K)`.
        # Output shape: `(B, 1, Hid)`.
        output = output.transpose(1, 2).contiguous()
        output = output.view(B, -1, self.n_head * self.d_k)

        # Output shape: `(B, Hid)`.
        return self.linear(output.squeeze(1))


class Encoder(nn.Module):
    def __init__(self, d_emb: int, p_hid: float):
        super().__init__()
        self.linear = nn.Linear(d_emb, d_emb)
        self.pe = PositionalEncoding(d_emb, p_hid)
        self.attn_emb = MultiHeadAttention(d_emb, 4)
        self.layernorm1 = nn.LayerNorm(d_emb)
        self.layernorm2 = nn.LayerNorm(d_emb)

    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        # Shape: [B, S, H]
        emb = self.layernorm1(self.linear(x))

        # Shape: [B, S, H]
        emb = self.pe(emb)

        # Shape: [B, H]
        emb = self.layernorm2(self.attn_emb(emb, mask))

        return emb

"""# Risk Classificaion

## Take a look at the risk classification dataset
"""

import os
import pandas as pd
import json
from dataset import dataset_risk

df = pd.read_csv("/content/Test_risk_classification.csv")
df["label"] = 0
df.loc[[1],['label']] = 1
df.to_csv("/content/Test_risk_classification.csv", index=False)


risk_file=os.path.join("data", "/content/Test_risk_classification.csv")
print(type(risk_file))
print('risk data:')
print(pd.read_csv(risk_file,usecols=['article_id','text','label']))
print('\n---------------------------------')
print('vocab:')
print(list(json.load(open(os.path.join("data", "vocab.json"))).items())[:10])

dataset = dataset_risk(
    vocab_path=os.path.join("data", "vocab.json"),
    risk_file=risk_file,
)
d = next(iter(dataset))
print('\n---------------------------------')
print('text\n', d['article'])
print(d['article'].shape)
r = {v: k for k, v in json.load(open(os.path.join("data", "vocab.json"))).items()}
print(*[''.join(map(lambda x:r[x], i))[:40] for i in d['article'][:4,:] ], sep='\n')
print('\n---------------------------------')
print('answer:\n', d['risk_answer'])

from google.colab import drive
drive.mount('/content/drive')

"""## Risk Classification model"""

class Risk_Classifier(nn.Module):
    def __init__(self, d_emb: int, p_drop: float, n_layers: int):
        super().__init__()
        hid = []
        self.l0 = nn.Linear(d_emb, d_emb)
        for _ in range(n_layers):
            hid.append(nn.Linear(in_features=d_emb, out_features=d_emb))
            hid.append(nn.ReLU())
            hid.append(nn.Dropout(p=p_drop))
        self.hid = nn.Sequential(*hid)
        self.l1 = nn.Linear(d_emb, d_emb//2)
        self.dropout = nn.Dropout(p_drop)
        self.l2 = nn.Linear(d_emb//2, 1)

    def forward(self, document: torch.Tensor) -> torch.Tensor:
        output = document
        output = self.l0(output)

        output = self.hid(output)
        #　Linear layer
        # Input shape: `(B, E)`
        # Ouput shape: `(B, E//2)`
        # output = F.relu(self.l1(document))
        output = F.relu(self.l1(output))

        #　Dropout
        # Input shape: `(B, E//2)`
        # Ouput shape: `(B, E//2)`
        # output = self.dropout(output)

        #　Linear layer
        # Input shape: `(B, E//2)`
        # Ouput shape: `(B, 1)`
        output = torch.sigmoid(self.l2(output))

        return output.squeeze(-1)


class risk_model(nn.Module):
    def __init__(self, embedding_path: str, d_emb: int, n_cls_layers: int, p_drop: float):
        super().__init__()
        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(
            np.load(embedding_path)), freeze=True, padding_idx=0)
        self.word_encoder = Encoder(d_emb, p_drop)
        self.encoder = Encoder(d_emb, p_drop)
        self.risk = Risk_Classifier(d_emb, p_drop, n_cls_layers)

    def forward(self, document):
        # Embedding layer
        # Shape: [B, `max_doc_len`, `max_sent_len`, E]
        doc = self.embedding(document)
        w_mask, s_mask = self.create_mask(document)

        # Sentence embedding
        # Input shape: [B, `max_doc_len`, `max_sent_len`, E]
        # Output shape: [B, `max_doc_len`, E]
        doc = torch.stack([ self.word_encoder(d,w) for d,w in zip(doc, w_mask)])

        # Document embedding
        # Input shape: [B, `max_doc_len`, E]
        # Output shape: [B, E]
        doc = self.encoder(doc, s_mask)

        risk_output = self.risk(doc)

        return risk_output

    @staticmethod
    def create_mask(batch_prev_tkids: torch.Tensor) -> torch.Tensor:
        # Create padding self attention masks.
        # Shape: [B, `max_doc_len`, `max_sent_len`, 1]
        # Output dtype: `torch.bool`.
        w_pad_mask = batch_prev_tkids == 0
        w_pad_mask = w_pad_mask.unsqueeze(-1)

        s_pad_mask = batch_prev_tkids.sum(dim=-1)
        s_pad_mask = s_pad_mask == 0
        s_pad_mask = s_pad_mask.unsqueeze(-1)

        return w_pad_mask, s_pad_mask

    def loss_fn(self, document, risk):
        pred_risk = self(document)
        pred_risk = pred_risk.reshape(-1)
        risk = risk.reshape(-1)
        return F.binary_cross_entropy(pred_risk, risk)

"""## Training Risk Classifier"""

import csv
import os
import pathlib
import random
import re

import numpy as np
from tqdm import tqdm
import torch
from torch.utils.data import DataLoader
from sklearn.metrics import roc_auc_score

def risk_train(model_cfg, dataset, device,  # model and datasets
               p_drop, n_epoch, batch_size, learning_rate,  # training hyper parameter
               save_step, model_path):  # saving model

    model = risk_model(**model_cfg, p_drop=p_drop).train().to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    dataldr = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    def save_model(md, step):
        torch.save(md.state_dict(), os.path.join(
            model_path, f"model-{step}.pt"))

    # Train loop
    step = 0
    for epoch in range(n_epoch):
        tqdm_dldr = tqdm(dataldr)

        avg_loss = 0
        for epoch_step, batch_data in enumerate(tqdm_dldr):
            optimizer.zero_grad()

            batch_document = batch_data["article"].to(device)
            batch_risk = batch_data["risk_answer"].to(device)

            loss = model.loss_fn(batch_document, batch_risk)
            loss.backward()
            optimizer.step()

            step += 1
            avg_loss += loss
            tqdm_dldr.set_description(
                f"epoch:{epoch}, loss:{avg_loss / (epoch_step+1):.04f}")

            if step % save_step == 0:
                save_model(model, step)

    save_model(model, step)





random_seed = 42
# Set random states for reproducibility
random.seed(random_seed)
np.random.seed(random_seed)
torch.manual_seed(random_seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(random_seed)
# Use cuda when possible
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Save training configuration
risk_model_path = os.path.join("exp", "risk")
pathlib.Path(risk_model_path).mkdir(parents=True, exist_ok=True)


model_cfg = {
    "embedding_path": os.path.join("data", "embeddings.npy"),
    "d_emb": 300,
    "n_cls_layers": 2,
}

dataset = dataset_risk(
    vocab_path=os.path.join("data", "vocab.json"),
    risk_file=os.path.join("data", "/content/Train_risk_classification_ans.csv"),
)

risk_train(
    model_cfg=model_cfg,
    dataset=dataset,
    model_path=risk_model_path,
    device=device,
    # Hyperparameters
    batch_size=4,
    learning_rate=1e-4,
    n_epoch=50,
    save_step=500,
    p_drop=0.1,
)

"""## Test on trained model"""

def save_result(output_path: str, data: list, ckpt: int):
    output = [["id", "label"]] + \
        [[i+1, label] for i, label in enumerate(data)]
    csv.writer(open(os.path.join(
        output_path, f"decision_{ckpt}.csv"), 'w', newline='')).writerows(output)
        
@torch.no_grad()
def risk_test(model_cfg, dataset, device, batch_size,
              model_path, output_path):
    dataldr = torch.utils.data.DataLoader(
        dataset, batch_size=batch_size, shuffle=False)

    # Load all checkpoints
    ckpts = sorted([
        (int(ckpt.group(1)), os.path.join(risk_model_path, ckpt.group(0)))
        for ckpt in map(lambda f:re.match(r'model-(\d+).pt', f), os.listdir(model_path))
        if ckpt is not None
    ], key=lambda x: x[0])

    for step, ckpt in ckpts:
        # Model
        model = risk_model(**model_cfg, p_drop=0.0)
        model.load_state_dict(torch.load(ckpt))
        model = model.eval().to(device)

        preds = []

        for batch_data in tqdm(dataldr):
            batch_document = batch_data["article"].to(device)
            preds += model(batch_document).tolist()
        print(f"\nroc_auc {step} : {roc_auc_score(dataset.risk, preds):.04f}", flush=True)
        save_result(output_path, preds, step)

risk_output_path = os.path.join("output", "risk")
pathlib.Path(risk_output_path).mkdir(parents=True, exist_ok=True)

dataset = dataset_risk(
    vocab_path=os.path.join("data", "vocab.json"),
    risk_file=os.path.join("data", "/content/Test_risk_classification.csv"),
)


print("\nevaluate on training set...", flush=True)
risk_test(
    model_cfg=model_cfg,
    dataset=dataset,
    model_path=risk_model_path,
    device=device,
    batch_size=8,
    output_path=risk_output_path,
)

"""## Preditions"""

risk_latest_result = sorted([
    (int(ckpt.group(1)), os.path.join(risk_output_path, ckpt.group(0)))
    for ckpt in map(lambda f:re.match(r'decision_(\d+).csv', f), os.listdir(risk_output_path))
    if ckpt is not None
], key=lambda x: x[0])[-1]

pd.read_csv(open(risk_latest_result[1]))

"""# Question Answer

## About Question Answer dataset
"""

from pprint import pprint
from dataset import dataset_qa

qa_file=os.path.join("data", "/content/Train_qa_ans.json")

print('qa data:')
pprint(json.load(open(qa_file))[0])

dataset = dataset_qa(
    vocab_path=os.path.join("data", "vocab.json"),
    qa_file=qa_file,
)

d = next(iter(dataset))
print('\n---------------------------------')
print('encoded article')
print(d['article'])
print(d['article'].shape)
print('\n---------------------------------')
print('encoded question')
print(d['question'])
print(d['question'].shape)
print('\n---------------------------------')
print('encoded choice')
print(d['choice'])
print(d['choice'].shape)
print('\n---------------------------------')
print('answer')
print(d['qa_answer'])

"""## Question Answer Model"""

class QA_Classifier(nn.Module):
    def __init__(self, d_emb: int, p_hid: float, n_layers: int):
        super().__init__()
        self.l1 = nn.Linear(3*d_emb, d_emb)
        self.dropout = nn.Dropout(p_hid)

        hid = []
        for _ in range(n_layers):
            hid.append(nn.Linear(in_features=d_emb, out_features=d_emb))
            hid.append(nn.ReLU())
            hid.append(nn.Dropout(p=p_hid))
        self.hid = nn.Sequential(*hid)
        self.l2 = nn.Linear(d_emb, 1)

    def forward(
        self,
        document: torch.Tensor,
        question: torch.Tensor,
        choice: torch.Tensor
    ) -> torch.Tensor:
        # Concatenates `document embedding`, `question embedding`
        # and `choice embeding`
        # Input shape: `(B, E)`, `(B, E)`, `(B, E)`
        # Ouput shape: `(B, 3*E)`
        output = torch.cat((document, question, choice), -1)

        #　Linear layer
        # Input shape: `(B, 3*E)`
        # Ouput shape: `(B, E)`
        output = F.relu(self.l1(output))

        #　Dropout
        # Input shape: `(B, E)`
        # Ouput shape: `(B, E)`
        output = self.dropout(output)

        # Hidden layer
        output = self.hid(output)

        #　Linear layer
        # Input shape: `(B, E)`
        # Ouput shape: `(B, 1)`
        output = torch.sigmoid(self.l2(output))

        return output


class qa_model(nn.Module):
    def __init__(self, embedding_path: str, d_emb: int, n_cls_layers: int, p_drop: float):
        super().__init__()
        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(
            np.load(embedding_path)), freeze=True, padding_idx=0)
        self.word_encoder = Encoder(d_emb, p_drop)
        self.encoder = Encoder(d_emb, p_drop)
        self.qa = QA_Classifier(d_emb, p_drop, n_cls_layers)

    def forward(self, document, question, choice):
        # Embedding layer
        # Shape: [B, `max_doc_len`, `max_sent_len`, E]
        doc = self.embedding(document)
        # Shape: [B, `max_q_len`, E]
        qst = self.embedding(question)
        # Shape: [B, 3, `max_c_len`, E]
        chs = self.embedding(choice)

        # Sentence embedding
        # Shape: [B, `max_doc_len`, E]
        w_mask, s_mask = self.create_mask(document)
        
        doc = torch.stack([self.word_encoder(d, m) for d, m in zip(doc, w_mask)])

        # Shape: [B, E]
        w_mask, _ = self.create_mask(question)
        qst = self.word_encoder(qst, w_mask)

        # Document embedding
        # Input shape: [B, `max_doc_len`, E]
        # Output shape: [B, E]
        doc = self.encoder(doc, s_mask)

        # Input Shape: [3, B, E]
        # Output Shape: [[B],[B],[B]]
        chs = chs.transpose(0, 1)
        w_mask, _ = self.create_mask(choice.transpose(0, 1))
        qa_output = [self.qa(doc, qst, self.word_encoder(ci, wmi)) for ci, wmi in zip(chs, w_mask)]
        qa_output = torch.cat(qa_output, dim=-1)
        return qa_output

    def create_mask(self, batch_prev_tkids: torch.Tensor) -> torch.Tensor:
        # Create padding self attention masks.
        # Shape: [B, `max_doc_len`, `max_sent_len`, 1]
        # Output dtype: `torch.bool`.
        w_pad_mask = batch_prev_tkids == 0
        w_pad_mask = w_pad_mask.unsqueeze(-1)

        s_pad_mask = batch_prev_tkids.sum(dim=-1)
        s_pad_mask = s_pad_mask == 0
        s_pad_mask = s_pad_mask.unsqueeze(-1)

        return w_pad_mask, s_pad_mask

    def loss_fn(self, document, question, choice, qa):
        pred_qa = self(document, question, choice)
        pred_qa = pred_qa.reshape(-1)
        qa = qa.reshape(-1)
        return F.binary_cross_entropy(pred_qa, qa)

"""## Training Question Answer Model"""

def qa_train(model_cfg, dataset, device,  # model and datasets
             p_drop, n_epoch, batch_size, learning_rate,  # training hyper parameter
             save_step, model_path):  # saving model

    model = qa_model(**model_cfg, p_drop=p_drop).train().to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    dataldr = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    def save_model(md, step):
        torch.save(md.state_dict(), os.path.join(
            model_path, f"model-{step}.pt"))

    # Train loop
    step = 0
    for epoch in range(n_epoch):
        tqdm_dldr = tqdm(dataldr)

        avg_loss = 0
        for epoch_step, batch_data in enumerate(tqdm_dldr):
            optimizer.zero_grad()
            loss = model.loss_fn(
                document=torch.LongTensor(batch_data['article']).to(device),
                question=batch_data["question"].to(device),
                choice=batch_data["choice"].to(device),
                qa=batch_data["qa_answer"].float().to(device))
            loss.backward()
            optimizer.step()

            step += 1
            avg_loss += loss
            tqdm_dldr.set_description(
                f"epoch:{epoch}, loss:{avg_loss / (epoch_step+1):.04f}")

            if step % save_step == 0:
                save_model(model, step)

    save_model(model, step)


random_seed = 42
# Set random states for reproducibility
random.seed(random_seed)
np.random.seed(random_seed)
torch.manual_seed(random_seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(random_seed)
# Use cuda when possible
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Save training configuration
qa_model_path = os.path.join("exp", "qa")
pathlib.Path(qa_model_path).mkdir(parents=True, exist_ok=True)

dataset = dataset_qa(
    vocab_path=os.path.join("data", "vocab.json"),
    qa_file=os.path.join("data", "/content/Train_qa_ans.json"),
)

model_cfg = {
    "embedding_path": os.path.join("data", "embeddings.npy"),
    "d_emb": 300,
    "n_cls_layers": 2,
}

qa_train(
    model_cfg=model_cfg,
    dataset=dataset,
    model_path=qa_model_path,
    device=device,
    # Hyperparameters
    batch_size=4,
    learning_rate=1e-4,
    n_epoch=10,
    save_step=18,
    p_drop=0.0,
)

"""## Test on trained QA model"""

from sklearn.metrics import accuracy_score

def qa_test(model_cfg, dataset, device, batch_size,
            model_path, output_path):
    dataldr = torch.utils.data.DataLoader(
        dataset, batch_size=batch_size, shuffle=False)

    # Load all checkpoints
    ckpts = sorted([
        (int(ckpt.group(1)), os.path.join(model_path, ckpt.group(0)))
        for ckpt in map(lambda f:re.match(r'model-(\d+).pt', f), os.listdir(model_path))
        if ckpt is not None
    ], key=lambda x: x[0])

    for step, ckpt in ckpts:
        model = qa_model(**model_cfg, p_drop=0.0)
        model.load_state_dict(torch.load(ckpt))
        model = model.eval().to(device)

        answer = []
        preds = []
        for batch_data in tqdm(dataldr):
            answer += batch_data["qa_answer"].argmax(dim=-1).tolist()
            pred_qa = model(
                document=torch.LongTensor(batch_data['article']).to(device),
                question=batch_data["question"].to(device),
                choice=batch_data["choice"].to(device))
            preds += pred_qa.argmax(dim=-1).tolist()

        print(f"\nstep {step} accuracy: {accuracy_score(answer, preds):.04f}", flush=True)
        save_result(output_path, preds, step)

qa_output_path = os.path.join("output", "qa")
pathlib.Path(qa_output_path).mkdir(parents=True, exist_ok=True)

print("\nevaluate on training set...", flush=True)
qa_test(
    model_cfg=model_cfg,
    dataset=dataset,
    model_path=qa_model_path,
    device=device,
    batch_size=8,
    output_path=qa_output_path,
)

"""## Preditions"""

qa_latest_result = sorted([
    (int(ckpt.group(1)), os.path.join(qa_output_path, ckpt.group(0)))
    for ckpt in map(lambda f:re.match(r'decision_(\d+).csv', f), os.listdir(qa_output_path))
    if ckpt is not None
], key=lambda x: x[0])[-1]

pd.read_csv(open(qa_latest_result[1]))